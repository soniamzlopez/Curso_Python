{"cells":[{"cell_type":"markdown","metadata":{"id":"G0LOaPyAgCU8"},"source":["#  RDDs"]},{"cell_type":"markdown","metadata":{"id":"yxqIrsY-gCVB"},"source":["# Creamos un contexto para crear RDDs"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"T3SkkrkIgCVD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697622997654,"user_tz":-120,"elapsed":34302,"user":{"displayName":"Sonia Muñoz López","userId":"01163762127533066237"}},"outputId":"2c3eff97-ec91-4cd6-cd39-10334d30230e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install pyspark --quiet\n","from pyspark import SparkContext"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_X-TkHs-gCVE","executionInfo":{"status":"ok","timestamp":1697623032097,"user_tz":-120,"elapsed":5059,"user":{"displayName":"Sonia Muñoz López","userId":"01163762127533066237"}}},"outputs":[],"source":["sc = SparkContext(master = \"local\", appName=\"TransformacionesyAcciones\")"]},{"cell_type":"markdown","metadata":{"id":"oYNeSv5EgCVF"},"source":["# Un RDD es una colección inmutable y distribuida de elementos\n","\n","### Spark automaticamente distribuye los datos y paraleliza las operaciones\n","\n","### Los RDD realmente cargan colecciones de datos"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"-yfNRIqPgCVF","executionInfo":{"status":"ok","timestamp":1697623050388,"user_tz":-120,"elapsed":895,"user":{"displayName":"Sonia Muñoz López","userId":"01163762127533066237"}}},"outputs":[],"source":["rdd1 = sc.parallelize([1,2,3])"]},{"cell_type":"markdown","metadata":{"id":"7daw3WsugCVF"},"source":["### Debido a la propiedad de distribución que tienen los RDD, en su creación, podemos particionar los datos"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"gV_9CtH-gCVG","executionInfo":{"status":"ok","timestamp":1697623058997,"user_tz":-120,"elapsed":433,"user":{"displayName":"Sonia Muñoz López","userId":"01163762127533066237"}}},"outputs":[],"source":["import numpy as np\n","rdd2 = sc.parallelize(np.array(range(100)),2)"]},{"cell_type":"markdown","metadata":{"id":"OXf9xyz4gCVG"},"source":["### Verificamos el tipo de dato"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"-6IXNzw2gCVG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697623063091,"user_tz":-120,"elapsed":411,"user":{"displayName":"Sonia Muñoz López","userId":"01163762127533066237"}},"outputId":"96ca7eb4-0d66-4d02-c297-07eb6fec1429"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.rdd.RDD"]},"metadata":{},"execution_count":5}],"source":["type(rdd1)"]},{"cell_type":"markdown","metadata":{"id":"M9fdoJ87gCVG"},"source":["### Vemos el contenido\n","\n","Veremos distintas tecnicas apropiadas para ver el contenido de los RDDs y Dataframes"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"QzqUUjhCgCVH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697623073340,"user_tz":-120,"elapsed":1111,"user":{"displayName":"Sonia Muñoz López","userId":"01163762127533066237"}},"outputId":"591665be-03b2-4aef-98c2-5efc211d707d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 2, 3]"]},"metadata":{},"execution_count":6}],"source":["rdd1.collect()"]},{"cell_type":"markdown","metadata":{"id":"cvXwqRwdgCVH"},"source":["# Carga de un arhivo CSV"]},{"cell_type":"markdown","metadata":{"id":"OfQ67xw1gCVI"},"source":["### Cargamos un RDDs\n","\n","El método textFile busca el archivo en la ruta indicada\n","\n","Cambia el valor de la ruta para que apunte a la ruta donde tienes los datos"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"9AG-1zOcgCVI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697623284890,"user_tz":-120,"elapsed":18259,"user":{"displayName":"Sonia Muñoz López","userId":"01163762127533066237"}},"outputId":"d6c11268-60f2-4be1-aa3c-5be231b5e1ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","equiposOlimpicosRDD = sc.textFile(\"/content/drive/MyDrive/Colab Notebooks/RED.ES/Datos Ejercicios/M6/paises.csv\",2).map(lambda line : line.split(\",\"))"]},{"cell_type":"markdown","metadata":{"id":"FGKXqN4sgCVI"},"source":["### Vemos el contenido\n","\n","El método take es otro método existnte para poder visualizar el contenido de los RDDs"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"HA3o7cq1gCVI","colab":{"base_uri":"https://localhost:8080/","height":880},"executionInfo":{"status":"error","timestamp":1697623304146,"user_tz":-120,"elapsed":295,"user":{"displayName":"Sonia Muñoz López","userId":"01163762127533066237"}},"outputId":"51119863-146b-4c09-e4e6-967c621c2ff4"},"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-8ff8629e2858>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mequiposOlimpicosRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2820\u001b[0m         \"\"\"\n\u001b[1;32m   2821\u001b[0m         \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2822\u001b[0;31m         \u001b[0mtotalParts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2823\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5452\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5453\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5455\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o27.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/drive/MyDrive/Colab Notebooks/RED.ES/Datos Ejercicios/M6/paises.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/content/drive/MyDrive/Colab Notebooks/RED.ES/Datos Ejercicios/M6/paises.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 25 more\n"]}],"source":["equiposOlimpicosRDD.take(10)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"LfOUKBkvgCVJ","executionInfo":{"status":"ok","timestamp":1697623308454,"user_tz":-120,"elapsed":861,"user":{"displayName":"Sonia Muñoz López","userId":"01163762127533066237"}}},"outputs":[],"source":["sc.stop()"]},{"cell_type":"markdown","source":["## Ejercicio:\n","\n","Intenta leer un archivo csv o de texto y hacer alguna transformación.\n","\n","* Busca el archivo del quijote.txt por internet.\n","* Subelo a Google COlab e importalo con pyspark.\n","* Cuenta los caracteres del fichero."],"metadata":{"id":"a0DSWeq2xcEH"}},{"cell_type":"code","source":["#Inserta aquí tú código\n"],"metadata":{"id":"vSx9LCyjxo9f"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"1bB5s6wfR7BmWZitfFkhiFLH0YhleTOsP","timestamp":1697619237386},{"file_id":"https://github.com/terranigmark/curso-apache-spark-platzi/blob/master/2.%20Primer%20RDD.ipynb","timestamp":1672143190353}]}},"nbformat":4,"nbformat_minor":0}